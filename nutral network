import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split

# Step 1: Load and Prepare Data
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist['data'], mnist['target']
y = y.astype(np.uint8)

# Normalize the input data
X = X / 255.0

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert labels to one-hot encoding
def one_hot_encode(y):
    n_labels = len(np.unique(y))
    encoded = np.zeros((len(y), n_labels))
    for i in range(len(y)):
        encoded[i, y[i]] = 1
    return encoded

y_train_encoded = one_hot_encode(y_train)
y_test_encoded = one_hot_encode(y_test)

# Step 2: Define Neural Network Architecture
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.W1 = np.random.randn(input_size, hidden_size)
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size)
        self.b2 = np.zeros((1, output_size))
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def softmax(self, x):
        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    
    def forward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.probs = self.softmax(self.z2)
        return self.probs
    
    def backward(self, X, y, learning_rate=0.01):
        m = X.shape[0]
        delta3 = self.probs.copy()
        delta3[range(m), y] -= 1
        dW2 = np.dot(self.a1.T, delta3)
        db2 = np.sum(delta3, axis=0, keepdims=True)
        delta2 = np.dot(delta3, self.W2.T) * self.a1 * (1 - self.a1)
        dW1 = np.dot(X.T, delta2)
        db1 = np.sum(delta2, axis=0)
        
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
    
    def predict(self, X):
        return np.argmax(self.forward(X), axis=1)

# Step 3: Training the Neural Network
input_size = X_train.shape[1]
hidden_size = 128
output_size = 10
num_epochs = 1000
learning_rate = 0.1

nn = NeuralNetwork(input_size, hidden_size, output_size)

for epoch in range(num_epochs):
    nn.forward(X_train)
    nn.backward(X_train, y_train, learning_rate)
    
    if epoch % 100 == 0:
        loss = -np.mean(np.log(nn.probs[range(len(X_train)), y_train] + 1e-10))
        accuracy = np.mean(nn.predict(X_train) == y_train) * 100
        print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.2f}%')

# Step 4: Evaluate the Model
test_accuracy = np.mean(nn.predict(X_test) == y_test) * 100
print(f'Final Test Accuracy: {test_accuracy:.2f}%')
